{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ramandeep-quiz3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RamandeepKBagri/tensorFlow/blob/master/ramandeep_quiz3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zf_4_8GjGli",
        "colab_type": "code",
        "outputId": "27c6afb8-82d0-4900-c388-30153efb9039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "#mnist = input_data.read_data_sets('/tmp/MNIST_data', one_hot=True)\n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "\n",
        "W_h1 = tf.Variable(tf.random_normal([784, 512]))\n",
        "b_1  = tf.Variable(tf.random_normal([512]))\n",
        "h1   = tf.nn.sigmoid(tf.matmul(x, W_h1) + b_1)\n",
        "\n",
        "W_out = tf.Variable(tf.random_normal([512, 10]))\n",
        "b_out = tf.Variable(tf.random_normal([10]))\n",
        "y_    = tf.nn.softmax(tf.matmul(h1, W_out) + b_out)\n",
        "\n",
        "#cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(y_, y)\n",
        "cross_entropy = tf.reduce_sum(-y * tf.log(y_), 1)\n",
        "loss = tf.reduce_mean(cross_entropy)\n",
        "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# train\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.initialize_all_variables())\n",
        "\n",
        "  for i in range(1000):\n",
        "    batch_x, batch_y = mnist.train.next_batch(100)\n",
        "    sess.run(train_step, feed_dict={x: batch_x, y: batch_y})\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      train_accuracy = accuracy.eval(feed_dict={x: batch_x, y: batch_y})\n",
        "      print('step {0}, training accuracy {1}'.format(i, train_accuracy))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0905 22:30:32.785473 140360271566720 deprecation.py:323] From <ipython-input-1-497da53bc87c>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0905 22:30:32.787469 140360271566720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0905 22:30:32.790857 140360271566720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "W0905 22:30:32.888597 140360271566720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0905 22:30:33.184825 140360271566720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0905 22:30:33.187369 140360271566720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "W0905 22:30:33.346904 140360271566720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0905 22:30:33.793400 140360271566720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 0, training accuracy 0.2199999988079071\n",
            "step 100, training accuracy 0.4300000071525574\n",
            "step 200, training accuracy 0.5699999928474426\n",
            "step 300, training accuracy 0.699999988079071\n",
            "step 400, training accuracy 0.6499999761581421\n",
            "step 500, training accuracy 0.7300000190734863\n",
            "step 600, training accuracy 0.7900000214576721\n",
            "step 700, training accuracy 0.8399999737739563\n",
            "step 800, training accuracy 0.8199999928474426\n",
            "step 900, training accuracy 0.7900000214576721\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEUJGkscjUdH",
        "colab_type": "code",
        "outputId": "cdad5633-1776-4310-bae5-723da4e49dab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets('/tmp/MNIST_data', one_hot=True)\n",
        "#mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "\n",
        "W_h1 = tf.Variable(tf.random_normal([784, 512]))\n",
        "b_1  = tf.Variable(tf.random_normal([512]))\n",
        "h1   = tf.nn.sigmoid(tf.matmul(x, W_h1) + b_1)\n",
        "\n",
        "W_out = tf.Variable(tf.random_normal([512, 10]))\n",
        "b_out = tf.Variable(tf.random_normal([10]))\n",
        "y_    = tf.nn.softmax(tf.matmul(h1, W_out) + b_out)\n",
        "\n",
        "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y)\n",
        "#cross_entropy = tf.reduce_sum(-y * tf.log(y_), 1)\n",
        "loss = tf.reduce_mean(cross_entropy)\n",
        "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# train\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.initialize_all_variables())\n",
        "\n",
        "  for i in range(1000):\n",
        "    batch_x, batch_y = mnist.train.next_batch(100)\n",
        "    sess.run(train_step, feed_dict={x: batch_x, y: batch_y})\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      train_accuracy = accuracy.eval(feed_dict={x: batch_x, y: batch_y})\n",
        "      print('step {0}, training accuracy {1}'.format(i, train_accuracy))\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0905 22:46:15.611880 140360271566720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 0, training accuracy 0.10000000149011612\n",
            "step 100, training accuracy 0.09000000357627869\n",
            "step 200, training accuracy 0.10999999940395355\n",
            "step 300, training accuracy 0.11999999731779099\n",
            "step 400, training accuracy 0.17000000178813934\n",
            "step 500, training accuracy 0.14000000059604645\n",
            "step 600, training accuracy 0.09000000357627869\n",
            "step 700, training accuracy 0.15000000596046448\n",
            "step 800, training accuracy 0.11999999731779099\n",
            "step 900, training accuracy 0.05999999865889549\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PritblWsm-Hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''comparison of above two results are; \n",
        "Accuracy decreased much by using sigmoid cross entropy based on per iteration'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYt-WyNvne8_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "4dfad160-5bcf-4009-cc91-668cf494699c"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "#mnist = input_data.read_data_sets('/tmp/MNIST_data', one_hot=True)\n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "\n",
        "W_h1 = tf.Variable(tf.random_normal([784, 512]))\n",
        "b_1  = tf.Variable(tf.random_normal([512]))\n",
        "h1   = tf.nn.sigmoid(tf.matmul(x, W_h1) + b_1)\n",
        "\n",
        "W_out = tf.Variable(tf.random_normal([512, 10]))\n",
        "b_out = tf.Variable(tf.random_normal([10]))\n",
        "y_    = tf.nn.softmax(tf.matmul(h1, W_out) + b_out)\n",
        "\n",
        "#cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(y_, y)\n",
        "cross_entropy = tf.reduce_sum(-y * tf.log(y_), 1)\n",
        "loss = tf.reduce_mean(cross_entropy)\n",
        "train_step = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# train\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.initialize_all_variables())\n",
        "\n",
        "  for i in range(1000):\n",
        "    batch_x, batch_y = mnist.train.next_batch(100)\n",
        "    sess.run(train_step, feed_dict={x: batch_x, y: batch_y})\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      train_accuracy = accuracy.eval(feed_dict={x: batch_x, y: batch_y})\n",
        "      print('step {0}, training accuracy {1}'.format(i, train_accuracy))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "step 0, training accuracy 0.07000000029802322\n",
            "step 100, training accuracy 0.07000000029802322\n",
            "step 200, training accuracy 0.09000000357627869\n",
            "step 300, training accuracy 0.10000000149011612\n",
            "step 400, training accuracy 0.09000000357627869\n",
            "step 500, training accuracy 0.10999999940395355\n",
            "step 600, training accuracy 0.05000000074505806\n",
            "step 700, training accuracy 0.05000000074505806\n",
            "step 800, training accuracy 0.03999999910593033\n",
            "step 900, training accuracy 0.10000000149011612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJLXpnj8oSGi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "922a015d-3508-4d31-e8e2-1f91f9086708"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "#mnist = input_data.read_data_sets('/tmp/MNIST_data', one_hot=True)\n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "\n",
        "W_h1 = tf.Variable(tf.random_normal([784, 512]))\n",
        "b_1  = tf.Variable(tf.random_normal([512]))\n",
        "h1   = tf.nn.sigmoid(tf.matmul(x, W_h1) + b_1)\n",
        "\n",
        "W_out = tf.Variable(tf.random_normal([512, 10]))\n",
        "b_out = tf.Variable(tf.random_normal([10]))\n",
        "y_    = tf.nn.softmax(tf.matmul(h1, W_out) + b_out)\n",
        "\n",
        "#cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(y_, y)\n",
        "cross_entropy = tf.reduce_sum(-y * tf.log(y_), 1)\n",
        "loss = tf.reduce_mean(cross_entropy)\n",
        "train_step = tf.train.AdamOptimizer(0.2).minimize(loss)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# train\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.initialize_all_variables())\n",
        "\n",
        "  for i in range(1000):\n",
        "    batch_x, batch_y = mnist.train.next_batch(100)\n",
        "    sess.run(train_step, feed_dict={x: batch_x, y: batch_y})\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      train_accuracy = accuracy.eval(feed_dict={x: batch_x, y: batch_y})\n",
        "      print('step {0}, training accuracy {1}'.format(i, train_accuracy))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "step 0, training accuracy 0.3499999940395355\n",
            "step 100, training accuracy 0.07000000029802322\n",
            "step 200, training accuracy 0.05999999865889549\n",
            "step 300, training accuracy 0.07000000029802322\n",
            "step 400, training accuracy 0.05000000074505806\n",
            "step 500, training accuracy 0.10000000149011612\n",
            "step 600, training accuracy 0.12999999523162842\n",
            "step 700, training accuracy 0.10999999940395355\n",
            "step 800, training accuracy 0.09000000357627869\n",
            "step 900, training accuracy 0.14000000059604645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxx9CV1Eox5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Compare above two results;\n",
        "At step zer0 accuracy incresed by increaing the step reta by 0.2, \n",
        "but in other iterations at stepp 100 to 300 found increasing in both cases with very small change and in step 400 and step 500 it is approximetely same. \n",
        "then accuracy for rest of the step found slightly more by using lr 0.2 than lr 0.1. Overall accuracy got improved by using lr 0.2'''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}